# Apache Analytics with Pig

This bundle is a 6 node cluster designed to scale out. Built around Apache
Hadoop components, it contains the following units:

* 1 HDFS Master
* 1 YARN Master
* 3 Compute Slaves
* 1 Hadoop Client
  - 1 Pig (colocated on the Hadoop Client unit)

## Usage
Deploy this bundle using juju-quickstart:

    juju quickstart <bundle>

See `juju quickstart --help` for ways to specify the `<bundle>` argument.
Alternatively, you may deploy this bundle using juju-deployer:

    juju deployer -c </path/to/bundles.yaml> apache-analytics-pig

### Smoke test HDFS admin functionality
Once the deployment is complete and the cluster is running, ssh to the HDFS
Master unit:

    juju ssh hdfs-master/0

As the HDFS user, create a /user/ubuntu directory on the Hadoop file system.
The steps below verify HDFS functionality:

    sudo su $HDFS_USER
    hdfs dfs -mkdir -p /user/ubuntu
    hdfs dfs -chown ubuntu:ubuntu /user/ubuntu
    hdfs dfs -chmod -R 755 /user/ubuntu
    exit

### Smoke test YARN and MapReduce
Run the Terasort script from the Hadoop Client unit to generate and sort data.
The steps below verify YARN and MapReduce functionality:

    juju ssh client/0
    ~/terasort.sh
    exit

### Smoke test HDFS functionality from user space
From the Hadoop Client unit, delete the MapReduce output previously generated by
the Terasort script:

    juju ssh client/0
    hdfs dfs -rm -r /user/ubuntu/out_dir
    exit

### Smoke test Pig in Local Mode

Run pig in local mode from the Pig unit:

    juju ssh pig/0
    pig -x local
    quit
    exit

### Smoke test Pig in MapReduce Mode

From the Pig unit, test in MapReduce mode as follows:

    juju ssh pig/0
    hdfs dfs -copyFromLocal /etc/passwd /user/ubuntu/passwd
    echo "A = load '/user/ubuntu/passwd' using PigStorage(':');" > /tmp/test.pig
    echo "B = foreach A generate \$0 as id; store B into '/tmp/pig.out';" >> /tmp/test.pig
    pig -l /tmp/test.log /tmp/test.pig
    hdfs dfs -cat /tmp/pig.out/part-m-00000
    exit

## Scale Out Usage
This bundle was designed to scale out. To increase the amount of Compute
Slaves, you can add units to the compute-slave service. To add one unit:

    juju add-unit compute-slave

Or you can add multiple units at once:

    juju add-unit -n4 compute-slave

## Contact Information
* [Amir Sanjar](<amir.sanjar@canonical.com>)
* [Cory Johns](<cory.johns@canonical.com>)
* [Kevin Monroe](<kevin.monroe@canonical.com>)

## Help

- [Juju mailing list](https://lists.ubuntu.com/mailman/listinfo/juju)
- [Juju community](https://jujucharms.com/community)
